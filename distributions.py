#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Abstract

- Definition of various 1D/2D flow-based PDFs and likelihoods.
- Bayesian update with flow-based PDFs.
"""

## Packages
import numpy as np
from math_funcs import *
from scipy.stats import norm as univariate_normal


## Functions
def normal_scalar_PDF(x,mu,sigma):
    """ 
    Evaluate the univariate Gaussian PDF for a given x.
    
    Parameters
    ----------
    x : float or ndarray
        Input.
    
    Returns
    -------
    pdf_out : float or ndarray
        PDF output.
    """
    pdf_out = 1.0/(sigma*np.sqrt(2.0*np.pi))*np.exp(-(x-mu)**2.0/(2.0*sigma**2.0))
    return pdf_out

def lognormal_scalar_PDF(x,mu,sigma):
    """ 
    Evaluate the univariate lognormal PDF for a given x.
    
    Parameters
    ----------
    x : float or ndarray
        Input.
    
    Returns
    -------
    pdf_out : float or ndarray
        PDF output.
    """
    pdf_out = 1.0/(x*sigma*np.sqrt(2.0*np.pi))*np.exp(-(np.log(x)-mu)**2.0/(2.0*sigma**2.0))
    return pdf_out

def logit_normal_scalar_PDF(x,mu,sigma):
    """ 
    Evaluate the univariate logit-normal PDF for a given x.
    
    Parameters
    ----------
    x : float or ndarray
        Input.
    
    Returns
    -------
    pdf_out : float or ndarray
        PDF output.
    """
    pdf_out = 1.0/(x*(1.0-x)*sigma*np.sqrt(2.0*np.pi))*\
              np.exp(-(inv_logistic(x)-mu)**2.0/(2.0*sigma**2.0))
    return pdf_out

def univariate_cdf(x,mu,sigma,distr):
    """ 
    Calculates the CDFs of Gaussian and
    Gaussian-related univariate distributions.

    Parameters
    ----------
    x : float or ndarray
        Input.
    mu : float
        Mu parameter.
    sigma : float
        Sigma parameter.
    distr : string
        Distribution type 
        (e.g., normal,lognormal,logit_normal).
    
    Returns
    -------
    cdf_out : float or ndarray
        Output.
    """
    standardNorm_distr = univariate_normal(loc=0.0,scale=1.0)
    if distr == 'normal':
        y = (x-mu)/sigma
    elif distr == 'lognormal':
        y = (np.log(x)-mu)/sigma
    elif distr == 'logit_normal':
        y = (inv_logistic(x)-mu)/sigma
    cdf_out = standardNorm_distr.cdf(y)
    return cdf_out

def univariate_ppf(p,mu,sigma,distr):
    """ 
    Calculates the CDF inverse (PPF) of Gaussian and
    Gaussian-related univariate distributions.

    Parameters
    ----------
    p : float or ndarray
        Input (quantile value).
    mu : float
        Mu parameter.
    sigma : float
        Sigma parameter.
    distr : string
        Distribution type 
        (e.g., normal,lognormal,logit_normal).
    
    Returns
    -------
    ppf_out : float or ndarray
        Output.
    """
    standardNorm_distr = univariate_normal(loc=0.0,scale=1.0)
    tmp = mu + sigma*standardNorm_distr.ppf(p)
    if distr == 'normal':
        ppf_out = tmp
    elif distr == 'lognormal':
        ppf_out = np.exp(tmp)
    elif distr == 'logit_normal':
        ppf_out = logistic(tmp)
    return ppf_out

def loglogit_normal_PDF(x1_grid,x2_grid,MU,S):
    """ 
    Evaluate the loglogit-normal PDF for 
    a range of (x1,x2) values.

    Parameters
    ----------
    x1_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x1.
    x2_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x2.
    MU : ndarray [Nx,1]
        Mu parameter of the loglogit-normal PDF.
    S : ndarray [Nx,Nx]
        Sigma parameter of the loglogit-normal PDF.
    
    Returns
    -------
    LL_pdf_out : ndarray [x2_gridSize,x1_gridSize]
        Loglogit-normal PDF value at each x.
    """

    ## Get precision matrix
    sqrt_detS = np.sqrt(det_2by2(S))
    invS = inv_2by2(S)

    ## Form difference vector diff=xG-MU
    x1G_grid,x2G_grid = np.log(x1_grid),inv_logistic(x2_grid)
    diff_x1G_grid = x1G_grid-MU[0,0]
    diff_x2G_grid = x2G_grid-MU[1,0]
    diff_grid = np.stack((diff_x1G_grid,diff_x2G_grid),axis=2)

    ## Compute dot (tensor) products in the exponent of the PDF
    norm2_tmp = np.tensordot(diff_grid,invS,axes=(-1,0)) # like np.dot but more general; here the dot product is performed along the last dimension of diff_grid and the first dimension of invS.
    norm2 = np.einsum('...i,...i->...',norm2_tmp,diff_grid) # another generalization of np.dot; here the dot product is performed along the last dimensions of nor2_tmp and diff_grid; einsum stands for Eistein summation.

    ## Calculate the rest of the PDF
    LL_pdf_out = 1.0/(x1_grid*x2_grid*(1.0-x2_grid)*sqrt_detS*2.0*np.pi)*np.exp(-0.5*norm2)
    return LL_pdf_out

def likelihood_logScalarOb_LLprior(y,R,H,x1_grid,x2_grid):
    """ 
    A likelihood for a scalar observation generated by exp(yG). 
    Here yG comes from a Gaussian observation model with a loglogit-normal 
    true state x, i.e. yG = HxG+noise, where xG is the corresponding 
    true state in the latent space and noise~N(0,R).

    Parameters
    ----------
    y : float
        Observation.
    R : float
        Variance of the observation noise in the latent space.
    H : ndarray [1,Nx]
        Observaton operator.
    x1_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x1.
    x2_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x2.
    
    Returns
    -------
    likelihood_out : ndarray [x2_gridSize,x1_gridSize]
        Likelihood values.
    """
    x1G_grid,x2G_grid = np.log(x1_grid),inv_logistic(x2_grid)
    xG_grid = np.stack((x1G_grid,x2G_grid),axis=2) # [x2_gridSize,x1_gridSize,Nx]
    HxG_grid = np.einsum('ij,...ij->...i',H,xG_grid) # dot product along the last dimension Nx 
                                                     # resulting dim = [x2_gridSize,x1_gridSize]
    likelihood_out = 1.0/(y*np.sqrt(R*2.0*np.pi))*\
                     np.exp(-(np.log(y)-HxG_grid)**2.0/(2.0*R))
    return likelihood_out

def likelihood_biasedScalarOb_LLprior(y,ymin,R,H,x1_grid,x2_grid):
    """ 
    A likelihood for a scalar observation generated by exp(yG)+ymin. 
    Here yG comes from a Gaussian observation model with a loglogit-normal 
    true state x, i.e. yG = HxG+noise, where xG is the 
    corresponding true state in the latent space and noise~N(0,R).

    Parameters
    ----------
    y : float
        Observation.
    ymin : float
        Minimum value below which instrument cannot observe.
    R : float
        Variance of the observation noise in the latent space.
    H : ndarray [1,Nx]
        Observaton operator.
    x1_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x1.
    x2_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x2.
    
    Returns
    -------
    likelihood_out : ndarray [x2_gridSize,x1_gridSize]
        Likelihood values.
    """
    x1G_grid,x2G_grid = np.log(x1_grid),inv_logistic(x2_grid)
    xG_grid = np.stack((x1G_grid,x2G_grid),axis=2) # [x2_gridSize,x1_gridSize,Nx]
    HxG_grid = np.einsum('ij,...ij->...i',H,xG_grid) # dot product along the last dimension Nx 
                                                     # resulting dim = [x2_gridSize,x1_gridSize]
    likelihood_out = 1.0/((y-ymin)*np.sqrt(R*2.0*np.pi))*\
                     np.exp(-(np.log(y-ymin)-HxG_grid)**2.0/(2.0*R))
    return likelihood_out

def likelihood_adaptiveScalarOb_LLprior(y,yG,R,H,x1_grid,x2_grid):
    """ 
    A likelihood for a scalar observation generated either by
    y=exp(yG) [if yG<=0] or y=yG [if yG>0]. Here yG comes from 
    a Gaussian observation model with a loglogit-normal true state 
    x, i.e. yG = HxG+noise, where xG is the corresponding 
    true state in the latent space and noise~N(0,R). The likelihood induced by 
    this adaptive observation model is either lognormal or Gaussian, 
    respectively.

    Parameters
    ----------
    y,yG : floats
        Observation and its Gaussian equivalent.
    R : float
        Variance of the observation noise in the latent space.
    H : ndarray [1,Nx]
        Observaton operator.
    x1_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x1.
    x2_grid : ndarray [x2_gridSize,x1_gridSize]
        Possible state values along x2.
    
    Returns
    -------
    likelihood_out : ndarray [x2_gridSize,x1_gridSize]
        Likelihood values.
    """
    x1G_grid,x2G_grid = np.log(x1_grid),inv_logistic(x2_grid)
    xG_grid = np.stack((x1G_grid,x2G_grid),axis=2) # [x2_gridSize,x1_gridSize,Nx]
    HxG_grid = np.einsum('ij,...ij->...i',H,xG_grid) # dot product along the last dimension Nx 
                                                     # resulting dim = [x2_gridSize,x1_gridSize]
    if yG <= 0.0: # lognormal case
        likelihood_out = 1.0/(y*np.sqrt(R*2.0*np.pi))*\
                         np.exp(-(np.log(y)-HxG_grid)**2.0/(2.0*R))
    else: # Gaussian case
        likelihood_out = 1.0/(np.sqrt(R*2.0*np.pi))*\
                         np.exp(-(y-HxG_grid)**2.0/(2.0*R))
    return likelihood_out
 
def bayesian_update_2D(MUp,Sp,H,yG,R):
    """ 
    2D Bayesian update of the prior PDF parameters {MUp,Sp} 
    after observing y (either a scalar or a 2D vector).

    Parameters
    ----------
    MUp : ndarray [2,1]
        Prior Mu parameter.
    Sp : ndarray [2,2]
        Prior Sigma parameter.
    H : ndarray [Ny,2]
        Observaton operator.
    yG : float or ndarray [2]
        Gaussian equivalent of observed value.    
    R : ndarray [1,1] or [2,2]
        Covariance matrix from 
        observation model.
    
    Returns
    -------
    MUu : ndarray [2,1]
        Posterior Mu parameter.
    Su : ndarray [2,2]
        Posterior Sigma parameter.
    """
    SHt = Sp.dot(H.T)
    HSHt = np.dot(H.dot(Sp),H.T)
    K = SHt.dot(inv_2by2(R+HSHt)) # Kalman gain
    MUu = MUp + K.dot( yG-H.dot(MUp) )
    Nx = MUu.shape[0]
    Su = ( np.identity(Nx)-K.dot(H) ).dot(Sp)
    return MUu,Su

def loglogit_normal_PDF_slowVersion(x1,x2,MU,S):
    """ Evaluate the loglogit-normal PDF for different
    x=(x1,x2)^T values.

    Parameters
    ----------
    x1,x2 : floats
        Components of the state x=(x1,x2)^T.
    MU : ndarray [Nx,1]
        Mu parameter of the loglogit-normal PDF.
    S : ndarray [Nx,Nx]
        Sigma parameter of the loglogit-normal PDF.
    
    Returns
    -------
    LL_pdf_out : float
        Loglogit-normal PDF value at x.
    """
    sqrt_detS = np.sqrt(det_2by2(S))
    invS = inv_2by2(S)
    xG = np.array([np.log(x1),inv_logistic(x2)])
    xG.shape = (2,1) # enforce a column vector
    diff = xG-MU
    norm2_tmp = np.dot(diff.T,invS)
    norm2 = np.dot(norm2_tmp,diff)
    LL_pdf_out = 1.0/(x1*x2*(1.0-x2)*sqrt_detS*2.0*np.pi)*np.exp(-0.5*norm2)
    return LL_pdf_out

def likelihood_logScalarOb_LLprior_slowVersion(y,R,H,x1,x2):
    """ A likelihood for a scalar observation generated by exp(yG). 
    Here yG comes from a Gaussian observation model with a loglogit-normal 
    true state x=(x1,x2)^T, i.e. yG = HxG+noise, where xG is the 
    corresponding Gaussian true state and noise~N(0,R).

    Parameters
    ----------
    y : float
        Observation.
    R : ndarray [1,1]
        Variance of the Gaussian observation noise.
    H : ndarray [1,Nx]
        Observaton operator.
    x1,x2 : floats
        Components of the non-Gaussian true state x=(x1,x2)^T.
    
    Returns
    -------
    likelihood_out : float
        Likelihood value.
    """
    xG = np.array([np.log(x1),inv_logistic(x2)])
    HxG = H.dot(xG)
    likelihood_out = 1.0/(y*np.sqrt(R*2.0*np.pi))*\
                     np.exp(-(np.log(y)-HxG)**2.0/(2.0*R))
    return likelihood_out

def likelihood_biasedScalarOb_LLprior_slowVersion(y,ymin,R,H,x1,x2):
    """ A likelihood for a scalar observation generated by exp(yG)+ymin. 
    Here yG comes from a Gaussian observation model with a loglogit-normal 
    true state x=(x1,x2)^T, i.e. yG = HxG+noise, where xG is the 
    corresponding Gaussian true state and noise~N(0,R).

    Parameters
    ----------
    y : float
        Observation.
    ymin : float
        Minimum value below which instrument cannot observe.
    R : ndarray [1,1]
        Variance of the Gaussian observation noise.
    H : ndarray [1,Nx]
        Observaton operator.
    x1,x2 : floats
        Components of the non-Gaussian true state x=(x1,x2)^T.
    
    Returns
    -------
    likelihood_out : float
        Likelihood value.
    """
    xG = np.array([np.log(x1),inv_logistic(x2)])
    HxG = H.dot(xG)
    likelihood_out = 1.0/((y-ymin)*np.sqrt(R*2.0*np.pi))*\
                     np.exp(-(np.log(y-ymin)-HxG)**2.0/(2.0*R))
    return likelihood_out

def likelihood_adaptiveScalarOb_LLprior_slowVersion(y,yG,R,H,x1,x2):
    """ A likelihood for a scalar observation generated either by
    y=exp(yG) [if yG<=0] or y=yG [if yG>0]. Here yG comes from 
    a Gaussian observation model with a loglogit-normal true state 
    x=(x1,x2)^T, i.e. yG = HxG+noise, where xG is the corresponding 
    Gaussian true state and noise~N(0,R). The likelihood induced by 
    this adaptive observation model is either lognormal or Gaussian, 
    respectively.

    Parameters
    ----------
    y,yG : floats
        Observation and its Gaussian equivalent.
    R : ndarray [1,1]
        Variance of the Gaussian observation noise.
    H : ndarray [1,Nx]
        Observaton operator.
    x1,x2 : floats
        Components of the non-Gaussian true state x=(x1,x2)^T.
    
    Returns
    -------
    likelihood_out : float
        Likelihood value.
    """
    xG = np.array([np.log(x1),inv_logistic(x2)])
    HxG = H.dot(xG)
    if yG <= 0.0: # lognormal case
        likelihood_out = 1.0/(y*np.sqrt(R*2.0*np.pi))*\
                         np.exp(-(np.log(y)-HxG)**2.0/(2.0*R))
    else: # Gaussian case
        likelihood_out = 1.0/(np.sqrt(R*2.0*np.pi))*\
                         np.exp(-(y-HxG)**2.0/(2.0*R))
    return likelihood_out
